{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.activations import  relu, hard_sigmoid\n",
    "from tensorflow.keras.layers import  Conv2D, ConvLSTM2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization , Concatenate\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from PIL import ImageOps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1000\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000005_image_02.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000005_image_02.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000008_image_03.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000008_image_03.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000011_image_02.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000011_image_02.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000014_image_03.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000014_image_03.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000017_image_02.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000017_image_02.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000020_image_03.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000020_image_03.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000023_image_02.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000023_image_02.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000026_image_03.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000026_image_03.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000029_image_02.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000029_image_02.png\n",
      "/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image/2011_09_26_drive_0002_sync_image_0000000032_image_03.png | /home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw/2011_09_26_drive_0002_sync_velodyne_raw_0000000032_image_03.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dir = \"/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/image\"\n",
    "target_dir = \"/home/roz/workspace/depth net/dataset/depth_selection/val_selection_cropped/velodyne_raw\"\n",
    "img_size = (256, 256)\n",
    "batch_size = 5\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".png\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/roz/workspace/depth net/Untitled-3.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Display input image #7\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m display(Image(filename\u001b[39m=\u001b[39minput_img_paths[\u001b[39m5\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Display auto-contrast version of corresponding target (per-pixel categories)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m that \u001b[39m=\u001b[39m ImageOps\u001b[39m.\u001b[39mautocontrast(load_img(target_img_paths[\u001b[39m5\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display input image #7\n",
    "display(Image(filename=input_img_paths[5]))\n",
    "\n",
    "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
    "that = ImageOps.autocontrast(load_img(target_img_paths[5]))\n",
    "display(that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roz/anaconda3/envs/NN/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "\n",
    "class OxfordPets(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "            #y[j] -= 1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/roz/workspace/depth net/Untitled-3.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_gen \u001b[39m=\u001b[39m OxfordPets(img_size, input_img_paths, target_img_paths)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mtype\u001b[39m(train_gen)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_size' is not defined"
     ]
    }
   ],
   "source": [
    "train_gen = OxfordPets(batch_size, img_size, input_img_paths, target_img_paths)\n",
    "type(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from keras.activations import hard_sigmoid, relu\n",
    "# from keras.layers import Conv2D\n",
    "# from keras.layers.convolutional import Conv2DTranspose\n",
    "# from keras.layers import ConvLSTM2D\n",
    "# from keras.layers.normalization.BatchNormalization_v1 import BatchNormalization\n",
    "#encoder\n",
    "#convolutional- lstm layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decon(filter): \n",
    "    return Conv2DTranspose(filters=filter, kernel_size=(3,3), strides= 1, padding= 'same', activation= relu)\n",
    "\n",
    "def con(filter, stride):\n",
    "    return Conv2D(filters=filter, kernel_size=(3,3), strides= (stride,stride))\n",
    "\n",
    "#input_image = Input(shape=(frames, channels, width, height))\n",
    "inputs = keras.Input( shape=(3,256,256,3),\n",
    "    batch_size=32,\n",
    "    name=None,\n",
    "    dtype=None,\n",
    "    sparse=None,\n",
    "    tensor=None,\n",
    "    ragged=None,\n",
    "    type_spec=None)\n",
    "input_ = inputs\n",
    "\n",
    "def depthnet(): \n",
    "\n",
    "    cnvlstm1= ConvLSTM2D(32, kernel_size=(7,7), strides=1, padding= 'same',  activation= relu,recurrent_activation=hard_sigmoid)(input_) #32\n",
    "    nm1=BatchNormalization(cnvlstm1)\n",
    "    cnvlstm2= ConvLSTM2D(64, kernel_size=(5,5), strides=2, padding= 'same',  activation= relu,recurrent_activation=hard_sigmoid)(nm1) #64\n",
    "    cnvlstm3= ConvLSTM2D(64, kernel_size=(5,5), strides=1, padding= 'same',  activation= relu,recurrent_activation=hard_sigmoid)(cnvlstm2) #64\n",
    "    nm2=BatchNormalization(cnvlstm3)\n",
    "    cnvlstm4= ConvLSTM2D(128, kernel_size=(3,3), strides=2, padding= 'same',  activation= relu,recurrent_activation=hard_sigmoid)(nm2) #128\n",
    "    cnvlstm5= ConvLSTM2D(128, kernel_size=(3,3), strides=1, padding= 'same',  activation= relu,recurrent_activation=hard_sigmoid)(cnvlstm4) #128\n",
    "    nm3=BatchNormalization(cnvlstm5)\n",
    "    cnvlstm6= ConvLSTM2D(256, kernel_size=(3,3), strides=2, padding= 'same',  activation= relu,recurrent_activation=hard_sigmoid)(nm3) #256\n",
    "    cnvlstm7= ConvLSTM2D(256, kernel_size=(3,3), strides=1, padding= 'same',  activation= relu,recurrent_activation=hard_sigmoid)(cnvlstm6)#256\n",
    "    nm4=BatchNormalization(cnvlstm7)\n",
    "    cnvlstm8= ConvLSTM2D(512, kernel_size=(3,3), strides=2, padding= 'same',  activation= relu,recurrent_activation=hard_sigmoid)(nm4)#512\n",
    "    nm5=BatchNormalization(cnvlstm8)\n",
    "    #decoder \n",
    "    dec1=  decon(512)(nm5)\n",
    "    conc1= Concatenate( [dec1 , nm5])\n",
    "    conv1= con(512, 2)\n",
    "    nm6= BatchNormalization(conv1)\n",
    "    dec2=  decon(256)(nm6)\n",
    "    conc2= Concatenate( [dec2 , nm4])\n",
    "    conv2= con(256, 1)\n",
    "    nm7= BatchNormalization(conv2)\n",
    "    dec3=  decon(128)(nm7)\n",
    "    conc3= Concatenate( [dec3 , nm3])\n",
    "    conv3= con(128, 2)\n",
    "    nm6= BatchNormalization(conv3)\n",
    "    dec4=  decon(64)(nm5)\n",
    "    conc4= Concatenate( [dec4 , nm2])\n",
    "    conv4= con(64, 1)\n",
    "    nm6= BatchNormalization(conv4)\n",
    "    dec5=  decon(32)(nm5)\n",
    "    conc5= Concatenate( [dec5 , nm1])\n",
    "    conv5= con(32, 2)\n",
    "    nm6= BatchNormalization(conv5)\n",
    "    dep = Conv2D(1, (1, 1), activation='sigmoid')(nm6)\n",
    "    net_model = Model(inputs= [input_] , outputs= [dep])\n",
    "    net_model.summary()\n",
    "    return net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 19:00:15.250502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-24 19:00:15.258154: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected an int or a list/tuple of ints for the argument 'axis', but received: <KerasTensor: shape=(32, 256, 256, 32) dtype=float32 (created by layer 'conv_lstm2d_8')>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/roz/workspace/depth net/Untitled-3.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m depthnet()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# loss function\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_loss_function\u001b[39m(output, target):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# di = output - target\u001b[39;00m\n",
      "\u001b[1;32m/home/roz/workspace/depth net/Untitled-3.ipynb Cell 8\u001b[0m in \u001b[0;36mdepthnet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdepthnet\u001b[39m(): \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     cnvlstm1\u001b[39m=\u001b[39m ConvLSTM2D(\u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m7\u001b[39m,\u001b[39m7\u001b[39m), strides\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m,  activation\u001b[39m=\u001b[39m relu,recurrent_activation\u001b[39m=\u001b[39mhard_sigmoid)(input_) \u001b[39m#32\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     nm1\u001b[39m=\u001b[39mBatchNormalization(cnvlstm1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     cnvlstm2\u001b[39m=\u001b[39m ConvLSTM2D(\u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m), strides\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, padding\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m,  activation\u001b[39m=\u001b[39m relu,recurrent_activation\u001b[39m=\u001b[39mhard_sigmoid)(nm1) \u001b[39m#64\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/roz/workspace/depth%20net/Untitled-3.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     cnvlstm3\u001b[39m=\u001b[39m ConvLSTM2D(\u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m), strides\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m,  activation\u001b[39m=\u001b[39m relu,recurrent_activation\u001b[39m=\u001b[39mhard_sigmoid)(cnvlstm2) \u001b[39m#64\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/NN/lib/python3.10/site-packages/keras/dtensor/utils.py:95\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m layout:\n\u001b[1;32m     93\u001b[0m       layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[0;32m---> 95\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/NN/lib/python3.10/site-packages/keras/layers/normalization/batch_normalization.py:1235\u001b[0m, in \u001b[0;36mBatchNormalization.__init__\u001b[0;34m(self, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mallow_initializer_layout\n\u001b[1;32m   1220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m   1221\u001b[0m              axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1233\u001b[0m              gamma_constraint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1234\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1235\u001b[0m   \u001b[39msuper\u001b[39;49m(BatchNormalization, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m   1236\u001b[0m       axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   1237\u001b[0m       momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m   1238\u001b[0m       epsilon\u001b[39m=\u001b[39;49mepsilon,\n\u001b[1;32m   1239\u001b[0m       center\u001b[39m=\u001b[39;49mcenter,\n\u001b[1;32m   1240\u001b[0m       scale\u001b[39m=\u001b[39;49mscale,\n\u001b[1;32m   1241\u001b[0m       beta_initializer\u001b[39m=\u001b[39;49mbeta_initializer,\n\u001b[1;32m   1242\u001b[0m       gamma_initializer\u001b[39m=\u001b[39;49mgamma_initializer,\n\u001b[1;32m   1243\u001b[0m       moving_mean_initializer\u001b[39m=\u001b[39;49mmoving_mean_initializer,\n\u001b[1;32m   1244\u001b[0m       moving_variance_initializer\u001b[39m=\u001b[39;49mmoving_variance_initializer,\n\u001b[1;32m   1245\u001b[0m       beta_regularizer\u001b[39m=\u001b[39;49mbeta_regularizer,\n\u001b[1;32m   1246\u001b[0m       gamma_regularizer\u001b[39m=\u001b[39;49mgamma_regularizer,\n\u001b[1;32m   1247\u001b[0m       beta_constraint\u001b[39m=\u001b[39;49mbeta_constraint,\n\u001b[1;32m   1248\u001b[0m       gamma_constraint\u001b[39m=\u001b[39;49mgamma_constraint,\n\u001b[1;32m   1249\u001b[0m       \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/NN/lib/python3.10/site-packages/keras/layers/normalization/batch_normalization.py:181\u001b[0m, in \u001b[0;36mBatchNormalizationBase.__init__\u001b[0;34m(self, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, renorm, renorm_clipping, renorm_momentum, fused, trainable, virtual_batch_size, adjustment, name, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m axis\n\u001b[1;32m    180\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpected an int or a list/tuple of ints for the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    182\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39margument \u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39maxis\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m, but received: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m axis)\n\u001b[1;32m    183\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39m=\u001b[39m momentum\n\u001b[1;32m    184\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m=\u001b[39m epsilon\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected an int or a list/tuple of ints for the argument 'axis', but received: <KerasTensor: shape=(32, 256, 256, 32) dtype=float32 (created by layer 'conv_lstm2d_8')>"
     ]
    }
   ],
   "source": [
    "model = depthnet()\n",
    "\n",
    "# loss function\n",
    "def custom_loss_function(output, target):\n",
    "    # di = output - target\n",
    "    di = target - output\n",
    "    n = (height * width)\n",
    "    di2 = np.power(di, 2)\n",
    "    fisrt_term = np.sum(di2,(1,2,3))/n\n",
    "    second_term = 0.5*np.power(np.sum(di,(1,2,3)), 2)/ (n**2)\n",
    "    loss = fisrt_term - second_term\n",
    "    return loss.mean()\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss=custom_loss_function(model.outputs , gtruth))\n",
    "\n",
    "#train_loader = DataGenerator(\n",
    "    #data=df[:260].reset_index(drop=\"true\"), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH))\n",
    "train_gen = OxfordPets(batch_size, img_size, input_img_paths, target_img_paths)\n",
    "epochs = 15\n",
    "model.fit(train_gen, epochs=epochs)\n",
    "#model.fit(train_loader,epochs=EPOCHS,validation_data=validation_loader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "626344daed0c42ae3048c35ad473a8619c79350fee44a24f4afbbc60690f3c8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
